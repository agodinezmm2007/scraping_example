{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "log_file_path = '/mnt/c/Users/WSTATION/Desktop/NEW_ETL_TEST/course_scraping_EHS.log'\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='w', \n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# add initial log entry\n",
    "logging.debug(\"Logging complete. Starting script execution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the target URL\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/List_of_colleges_and_universities_in_New_York_(state)\"\n",
    "logging.info(f\"Target URL set: {wiki_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fetch and parse the Wikipedia page\n",
    "try:\n",
    "    headers = {'User-Agent': 'InstitutionScraper/1.0 (agodinez@albany.edu)'}\n",
    "    response = requests.get(wiki_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    logging.info(\"Wikipedia page fetched and parsed successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error fetching or parsing the Wikipedia page: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Identify the desired sections\n",
    "desired_sections = [\n",
    "    'Public',\n",
    "    'Private, not-for-profit, non-sectarian'\n",
    "]\n",
    "logging.info(f\"Desired sections: {desired_sections}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract institutions from desired sections\n",
    "institutions = []\n",
    "\n",
    "# Find all divs with class 'mw-heading mw-heading2' (section headings)\n",
    "for div_h2 in soup.find_all('div', class_='mw-heading mw-heading2'):\n",
    "    h2 = div_h2.find('h2')\n",
    "    if h2:\n",
    "        section_title = h2.get_text(strip=True).replace('[edit]', '').strip()\n",
    "        logging.info(f\"Found section: '{section_title}'\")\n",
    "        if section_title in desired_sections:\n",
    "            logging.info(f\"Processing section: {section_title}\")\n",
    "\n",
    "            # Initialize sub_section_title\n",
    "            sub_section_title = ''\n",
    "\n",
    "            # Collect all elements until the next section heading\n",
    "            content = []\n",
    "            sibling = div_h2.find_next_sibling()\n",
    "            while sibling:\n",
    "                # Break if we reach another section heading\n",
    "                if sibling.name == 'div' and 'mw-heading2' in sibling.get('class', []):\n",
    "                    break\n",
    "                content.append(sibling)\n",
    "                sibling = sibling.find_next_sibling()\n",
    "\n",
    "            # Process the content\n",
    "            for element in content:\n",
    "                # Check for subsection headings\n",
    "                if element.name == 'div' and 'mw-heading3' in element.get('class', []):\n",
    "                    h3 = element.find('h3')\n",
    "                    if h3:\n",
    "                        sub_section_title = h3.get_text(strip=True).replace('[edit]', '').strip()\n",
    "                        logging.info(f\"  Sub-section: {sub_section_title}\")\n",
    "                else:\n",
    "                    # Find all <li> elements within the content\n",
    "                    for li in element.find_all('li'):\n",
    "                        a_tag = li.find('a', href=True)\n",
    "                        if a_tag:\n",
    "                            institution_name = a_tag.get_text(strip=True)\n",
    "                            institution_wiki_link = 'https://en.wikipedia.org' + a_tag['href']\n",
    "                            # Avoid duplicates\n",
    "                            if not any(inst['Name'] == institution_name for inst in institutions):\n",
    "                                institutions.append({\n",
    "                                    'Name': institution_name,\n",
    "                                    'Wikipedia Link': institution_wiki_link,\n",
    "                                    'Section': section_title,\n",
    "                                    'Sub-section': sub_section_title\n",
    "                                })\n",
    "                                logging.debug(f\"Added institution: {institution_name} from {sub_section_title}\")\n",
    "        else:\n",
    "            logging.debug(f\"Skipped section: {section_title}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fetch official website links from institution Wikipedia pages\n",
    "for idx, institution in enumerate(institutions):\n",
    "    logging.info(f\"Processing institution {idx + 1}/{len(institutions)}: {institution['Name']}\")\n",
    "    try:\n",
    "        # Use the requests library to get the institution's Wikipedia page\n",
    "        time.sleep(1)  \n",
    "        page_response = requests.get(institution['Wikipedia Link'], headers=headers)\n",
    "        page_response.raise_for_status()\n",
    "        page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "        # Find the infobox\n",
    "        infobox = page_soup.find('table', {'class': re.compile('infobox')})\n",
    "        if infobox:\n",
    "            # Look for the website link\n",
    "            website = ''\n",
    "            for row in infobox.find_all('tr'):\n",
    "                if row.th and ('Website' in row.th.get_text() or 'website' in row.th.get_text()):\n",
    "                    if row.td:\n",
    "                        link = row.td.find('a', href=True)\n",
    "                        if link and 'href' in link.attrs:\n",
    "                            website = link['href']\n",
    "                            logging.debug(f\"Found website for {institution['Name']}: {website}\")\n",
    "                    break\n",
    "            institution['Website'] = website\n",
    "        else:\n",
    "            logging.warning(f\"Infobox not found for {institution['Name']}\")\n",
    "            institution['Website'] = ''\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for {institution['Name']}: {e}\")\n",
    "        institution['Website'] = ''\n",
    "\n",
    "logging.info(\"Website extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save the data to a CSV file\n",
    "try:\n",
    "    df = pd.DataFrame(institutions)\n",
    "    output_path = '/mnt/c/Users/WSTATION/Desktop/NEW_ETL_TEST/ny_institutions.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Data saved to {output_path}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving data to CSV: {e}\")\n",
    "    raise\n",
    "logging.info(\"Script execution completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(institutions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
