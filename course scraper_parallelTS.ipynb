{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import os\n",
    "import concurrent.futures  # Added for parallel processing\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "log_file_path = 'C:/Users/alex/OneDrive - University at Albany - SUNY/FALL 2024/graduate assistant/course_catalog_extraction_P_MAIN_TS.log'\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='w',\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Add initial log entry\n",
    "logging.debug(\"Logging complete. Starting script execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_course_catalog_url(base_url):\n",
    "    logging.info(f\"Attempting to find course catalog for {base_url}\")\n",
    "    # Parse the base URL to ensure correctness\n",
    "    parsed_url = urlparse(base_url)\n",
    "    scheme = parsed_url.scheme or 'https'\n",
    "    netloc = parsed_url.netloc or parsed_url.path\n",
    "    if not netloc:\n",
    "        logging.error(f\"Invalid base URL: {base_url}\")\n",
    "        return {'catalog_url': None, 'status': 'Invalid base URL'}\n",
    "    \n",
    "    # Remove 'www.' from netloc if present\n",
    "    netloc = netloc.replace('www.', '')\n",
    "    \n",
    "    # List of possible prefixes\n",
    "    prefixes = ['catalog', 'catalogs', 'course', 'courses']\n",
    "    \n",
    "    potential_urls = []\n",
    "    \n",
    "    # Generate potential URLs by adding prefixes as subdomains\n",
    "    for prefix in prefixes:\n",
    "        potential_url = f\"{scheme}://{prefix}.{netloc}\"\n",
    "        potential_urls.append(potential_url)\n",
    "    \n",
    "    logging.info(f\"Generated {len(potential_urls)} potential catalog URLs for {base_url}\")\n",
    "    \n",
    "    # Verify which URL leads to the course catalog\n",
    "    for url in potential_urls:\n",
    "        try:\n",
    "            logging.debug(f\"Trying URL: {url}\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                page_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                title = page_soup.title.string.lower() if page_soup.title else ''\n",
    "                if any(keyword in title for keyword in ['course', 'catalog', 'bulletin', 'curriculum']):\n",
    "                    logging.info(f\"Found course catalog URL: {url}\")\n",
    "                    return {'catalog_url': url, 'status': 'Catalog found via title'}\n",
    "                # Additional check: look for specific keywords in the page content\n",
    "                if page_soup.find(string=re.compile('course catalog', re.I)):\n",
    "                    logging.info(f\"Found course catalog URL by content match: {url}\")\n",
    "                    return {'catalog_url': url, 'status': 'Catalog found via content match'}\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Error accessing {url}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logging.warning(f\"No course catalog found for {base_url}\")\n",
    "    return {'catalog_url': None, 'status': 'No catalog found'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_courses(args):\n",
    "    # Unpack arguments\n",
    "    catalog_url, institution_name, base_url = args\n",
    "\n",
    "    import logging\n",
    "    import time\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.common.exceptions import (\n",
    "        NoSuchElementException, TimeoutException,\n",
    "        StaleElementReferenceException, ElementClickInterceptedException\n",
    "    )\n",
    "\n",
    "    # Configure logging for each thread\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=f\"%(asctime)s - %(levelname)s - [Thread {threading.current_thread().name}] - %(message)s\"\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Starting course search on {catalog_url} for {institution_name}\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Uncomment the following line to run Chrome in headless mode\n",
    "    # options.add_argument('--headless')\n",
    "\n",
    "    # Initialize the Chrome WebDriver\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"WebDriver initialization failed for {institution_name}: {e}\")\n",
    "        return {'courses': [], 'status': f'WebDriver init failed: {e}'}\n",
    "\n",
    "    courses = []\n",
    "\n",
    "    try:\n",
    "        # Navigate to the Advanced Search Page\n",
    "        advanced_search_url = catalog_url.rstrip('/') + '/search_advanced.php'\n",
    "        driver.get(advanced_search_url)\n",
    "        logging.info(f\"Navigated to advanced search page: {advanced_search_url}\")\n",
    "\n",
    "        wait = WebDriverWait(driver, 15)  # Increased timeout to 15 seconds\n",
    "\n",
    "        # Locate the 'database_search' form\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.NAME, 'database_search')))\n",
    "            database_search_form = driver.find_element(By.NAME, 'database_search')\n",
    "            logging.info(\"Located 'database_search' form\")\n",
    "        except TimeoutException:\n",
    "            logging.error(f\"'database_search' form not found on {advanced_search_url}\")\n",
    "            return {'courses': [], 'status': \"'database_search' form not found\"}\n",
    "\n",
    "        # Locate the Keyword Search Input within the form\n",
    "        try:\n",
    "            search_box = database_search_form.find_element(By.NAME, 'filter[keyword]')\n",
    "        except NoSuchElementException:\n",
    "            logging.error(f\"Keyword search box not found in 'database_search' form on {advanced_search_url}\")\n",
    "            return {'courses': [], 'status': \"Keyword search box not found\"}\n",
    "\n",
    "        # Fill in the search term\n",
    "        try:\n",
    "            search_term = 'environmental health'\n",
    "            search_box.clear()\n",
    "            search_box.send_keys(f'\"{search_term}\"')  # Ensure exact phrase search\n",
    "            logging.info(f\"Entered search term '{search_term}' on {advanced_search_url}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to enter search term on {advanced_search_url}: {e}\")\n",
    "            return {'courses': [], 'status': f\"Failed to enter search term: {e}\"}\n",
    "\n",
    "        # Ensure 'Exact Match' Checkbox is Checked within the form\n",
    "        try:\n",
    "            exact_match_checkbox = database_search_form.find_element(By.NAME, 'filter[exact_match]')\n",
    "            if not exact_match_checkbox.is_selected():\n",
    "                exact_match_checkbox.click()\n",
    "                logging.info(\"Checked 'Exact Match' checkbox\")\n",
    "        except NoSuchElementException:\n",
    "            logging.warning(f\"'Exact Match' checkbox not found on {advanced_search_url}\")\n",
    "            # Proceeding even if the checkbox is not found\n",
    "\n",
    "        # Set Search Categories within the form\n",
    "        # Check 'Courses' checkbox and uncheck others\n",
    "        categories = [\n",
    "            ('filter_course', True, 'Courses'),\n",
    "            ('filter_program', False, 'Programs'),\n",
    "            ('filter_entity', False, 'Schools and Colleges'),\n",
    "            ('filter_other', False, 'Academic Rules')\n",
    "        ]\n",
    "        for category_id, should_check, category_name in categories:\n",
    "            try:\n",
    "                category_checkbox = database_search_form.find_element(By.ID, category_id)\n",
    "                if category_checkbox.is_selected() != should_check:\n",
    "                    category_checkbox.click()\n",
    "                    action = \"Checked\" if should_check else \"Unchecked\"\n",
    "                    logging.info(f\"{action} '{category_name}' checkbox\")\n",
    "            except NoSuchElementException:\n",
    "                logging.debug(f\"'{category_name}' checkbox not found or could not be interacted with\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error interacting with '{category_name}' checkbox: {e}\")\n",
    "                return {'courses': [], 'status': f\"Error with '{category_name}' checkbox: {e}\"}\n",
    "\n",
    "        # Submit the 'database_search' Form\n",
    "        try:\n",
    "            database_search_form.submit()\n",
    "            logging.info(\"Submitted 'database_search' form\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to submit 'database_search' form: {e}\")\n",
    "            return {'courses': [], 'status': f\"Failed to submit form: {e}\"}\n",
    "\n",
    "        # Wait for and Extract Search Results\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'table.table_default')))\n",
    "            logging.info(\"Search results table found.\")\n",
    "        except TimeoutException:\n",
    "            logging.warning(f\"No search results found for {institution_name} on {advanced_search_url}\")\n",
    "            return {'courses': [], 'status': \"No search results found\"}\n",
    "\n",
    "        # Parse Course Entries\n",
    "        try:\n",
    "            results_table = driver.find_element(By.CSS_SELECTOR, 'table.table_default')\n",
    "            course_links = results_table.find_elements(By.XPATH, './/a[starts-with(@onclick, \"showCatalogData\") or starts-with(@onclick, \"showCourse\")]')\n",
    "            logging.info(f\"Found {len(course_links)} course links in the results table\")\n",
    "        except NoSuchElementException:\n",
    "            logging.warning(f\"No course links found in the results table for {institution_name}\")\n",
    "            return {'courses': [], 'status': \"No course links found\"}\n",
    "\n",
    "        # Extract all course links' texts and their onclick attributes first to avoid StaleElementReferenceException\n",
    "        course_data = []\n",
    "        for link in course_links:\n",
    "            try:\n",
    "                course_name = link.text.strip()\n",
    "                onclick_attr = link.get_attribute('onclick')\n",
    "                course_data.append({'name': course_name, 'onclick': onclick_attr})\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error extracting data from course link: {e}\")\n",
    "                continue\n",
    "\n",
    "        logging.debug(f\"Course data extracted: {course_data}\")\n",
    "\n",
    "        # Iterate through each course data\n",
    "        for index, course in enumerate(course_data, start=1):\n",
    "            course_name = course['name']\n",
    "            onclick = course['onclick']\n",
    "            logging.debug(f\"Processing course {index}: {course_name} with onclick: {onclick}\")\n",
    "\n",
    "            # Use updated regex to extract parameters from onclick\n",
    "            match = re.search(\n",
    "                r\"showCourse\\(\\s*'(\\d+)'\\s*,\\s*'(\\d+)'\\s*,\\s*this\\s*,\\s*'([^']*)'\\s*\\)\",\n",
    "                onclick,\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "            if not match:\n",
    "                logging.error(f\"Could not parse onclick attribute for course '{course_name}': {onclick}\")\n",
    "                continue\n",
    "\n",
    "            catoid, coid, display_options = match.groups()\n",
    "\n",
    "            # Construct the AJAX URL dynamically based on the catalog_url\n",
    "            ajax_url = (\n",
    "                f\"{catalog_url.rstrip('/')}/ajax/preview_course.php\"\n",
    "                f\"?catoid={catoid}&coid={coid}&display_options={display_options}&show\"\n",
    "            )\n",
    "            logging.debug(f\"Constructed AJAX URL for course '{course_name}': {ajax_url}\")\n",
    "\n",
    "            # Open the AJAX URL in a new tab\n",
    "            try:\n",
    "                driver.execute_script(\"window.open('');\")  # Open a new tab\n",
    "                driver.switch_to.window(driver.window_handles[-1])  # Switch to the new tab\n",
    "                driver.get(ajax_url)\n",
    "                logging.debug(f\"Navigated to AJAX URL: {ajax_url}\")\n",
    "\n",
    "                # Wait for the AJAX content to load\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "                time.sleep(1)  # Additional wait to ensure content is fully loaded\n",
    "\n",
    "                # Get the page source and parse with BeautifulSoup\n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Extract the course description based on the provided HTML structure\n",
    "                # Locate the <h3> with the course name\n",
    "                h3_heading = soup.find('h3', text=re.compile(re.escape(course_name), re.I))\n",
    "                if h3_heading:\n",
    "                    parent_div = h3_heading.find_parent('div')\n",
    "                    if parent_div:\n",
    "                        # Remove the <h3> tag\n",
    "                        h3_heading.decompose()\n",
    "                        # Replace <br> tags with newline characters for better formatting\n",
    "                        for br in parent_div.find_all('br'):\n",
    "                            br.replace_with('\\n')\n",
    "                        # Extract the text\n",
    "                        course_description = parent_div.get_text(separator=' ', strip=True)\n",
    "                    else:\n",
    "                        logging.warning(f\"No parent <div> found for <h3> in course '{course_name}'. Extracting all text.\")\n",
    "                        course_description = soup.get_text(separator=' ', strip=True)\n",
    "                else:\n",
    "                    logging.warning(f\"<h3> heading with course name '{course_name}' not found. Extracting all text.\")\n",
    "                    course_description = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "                logging.debug(f\"Extracted description for course '{course_name}': {course_description[:100]}...\")  # Log first 100 chars\n",
    "\n",
    "                # Append the course to the list\n",
    "                courses.append({\n",
    "                    'Institution': institution_name,\n",
    "                    'Website': base_url,\n",
    "                    'Course Name': course_name,\n",
    "                    'Description': course_description\n",
    "                })\n",
    "\n",
    "                # Close the tab and switch back to the main window\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                # Brief pause before processing the next course\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error fetching description for course '{course_name}': {e}\")\n",
    "                # Ensure the new tab is closed if an error occurs\n",
    "                if len(driver.window_handles) > 1:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "                continue  # Skip to the next course\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {catalog_url}: {e}\")\n",
    "        return {'courses': [], 'status': f\"Error processing catalog: {e}\"}\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return {'courses': courses, 'status': 'Success'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_44068\\2980703037.py:184: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  h3_heading = soup.find('h3', text=re.compile(re.escape(course_name), re.I))\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "def main():\n",
    "    logging.info(\"Starting main execution\")\n",
    "    # Read the institutions from the CSV file\n",
    "    csv_file_path = 'C:/Users/alex/OneDrive - University at Albany - SUNY/FALL 2024/graduate assistant/ny_institutions.csv'\n",
    "    df_institutions = pd.read_csv(csv_file_path)\n",
    "\n",
    "    all_courses = []\n",
    "    failed_institutions = []\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for idx, row in df_institutions.iterrows():\n",
    "        institution_name = row['Name']\n",
    "        base_url = row['Website']\n",
    "\n",
    "        # Ensure the base URL is complete\n",
    "        if not base_url.startswith('http'):\n",
    "            base_url = 'https://' + base_url\n",
    "\n",
    "        logging.info(f\"Processing {institution_name} ({base_url})\")\n",
    "\n",
    "        catalog_info = find_course_catalog_url(base_url)\n",
    "        catalog_url = catalog_info['catalog_url']\n",
    "        catalog_status = catalog_info['status']\n",
    "\n",
    "        if catalog_url:\n",
    "            tasks.append((catalog_url, institution_name, base_url))\n",
    "        else:\n",
    "            # Log the failure\n",
    "            failed_institutions.append({\n",
    "                'Institution Name': institution_name,\n",
    "                'Base URL': base_url,\n",
    "                'Catalog URL Found': False,\n",
    "                'Failure Reason': catalog_status\n",
    "            })\n",
    "            logging.warning(f\"Course catalog not found for {institution_name}: {catalog_status}\")\n",
    "\n",
    "    # Use ThreadPoolExecutor to run tasks in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit all tasks and unpack arguments\n",
    "        future_to_task = {executor.submit(search_courses, task): task for task in tasks}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_task):\n",
    "            task = future_to_task[future]\n",
    "            institution_name = task[1]\n",
    "            base_url = task[2]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                courses = result.get('courses', [])\n",
    "                status = result.get('status', 'Unknown')\n",
    "\n",
    "                if status == 'Success' and courses:\n",
    "                    all_courses.extend(courses)\n",
    "                    logging.info(f\"Found {len(courses)} courses for {institution_name}\")\n",
    "                elif status == 'Success' and not courses:\n",
    "                    logging.info(f\"No courses found for {institution_name}\")\n",
    "                    failed_institutions.append({\n",
    "                        'Institution Name': institution_name,\n",
    "                        'Base URL': base_url,\n",
    "                        'Catalog URL Found': True,\n",
    "                        'Failure Reason': 'No courses found'\n",
    "                    })\n",
    "                else:\n",
    "                    # Status indicates failure\n",
    "                    failed_institutions.append({\n",
    "                        'Institution Name': institution_name,\n",
    "                        'Base URL': base_url,\n",
    "                        'Catalog URL Found': True,\n",
    "                        'Failure Reason': status\n",
    "                    })\n",
    "                    logging.error(f\"Failed to scrape courses for {institution_name}: {status}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {institution_name}: {e}\")\n",
    "                failed_institutions.append({\n",
    "                    'Institution Name': institution_name,\n",
    "                    'Base URL': base_url,\n",
    "                    'Catalog URL Found': True,\n",
    "                    'Failure Reason': f\"Unhandled exception: {e}\"\n",
    "                })\n",
    "\n",
    "    # Save the data to CSV files\n",
    "    try:\n",
    "        # Save successful courses\n",
    "        if all_courses:\n",
    "            df_courses = pd.DataFrame(all_courses)\n",
    "            output_file = 'C:/Users/alex/OneDrive - University at Albany - SUNY/FALL 2024/graduate assistant/scripts/environmental_health_courses_finalTS.csv'\n",
    "            df_courses.to_csv(output_file, index=False)\n",
    "            logging.info(f\"All courses saved to {output_file}\")\n",
    "        else:\n",
    "            logging.info(\"No courses found.\")\n",
    "\n",
    "        # Save failed institutions\n",
    "        if failed_institutions:\n",
    "            df_failed = pd.DataFrame(failed_institutions)\n",
    "            failed_output_file = 'C:/Users/alex/OneDrive - University at Albany - SUNY/FALL 2024/graduate assistant/scripts/failed_institutions.csv'\n",
    "            df_failed.to_csv(failed_output_file, index=False)\n",
    "            logging.info(f\"Failed institutions saved to {failed_output_file}\")\n",
    "        else:\n",
    "            logging.info(\"No failures encountered.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "    logging.info(\"Script execution completed.\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
